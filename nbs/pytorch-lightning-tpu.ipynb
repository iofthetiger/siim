{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Pytorch + TPU + Lightning"},{"metadata":{},"cell_type":"markdown","source":"> Pytorchüî• PyTroch - Lightning‚ö°Ô∏è TPU‚è±"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev","execution_count":1,"outputs":[{"output_type":"stream","text":"  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100  5115  100  5115    0     0   9283      0 --:--:-- --:--:-- --:--:--  9283\nUpdating... This may take around 2 minutes.\nUpdating TPU runtime to pytorch-nightly ...\nFound existing installation: torch 1.5.0\nUninstalling torch-1.5.0:\n  Successfully uninstalled torch-1.5.0\nFound existing installation: torchvision 0.6.0a0+35d732a\nUninstalling torchvision-0.6.0a0+35d732a:\nDone updating TPU runtime\n  Successfully uninstalled torchvision-0.6.0a0+35d732a\nCopying gs://tpu-pytorch/wheels/torch-nightly-cp37-cp37m-linux_x86_64.whl...\n| [1 files][110.1 MiB/110.1 MiB]                                                \nOperation completed over 1 objects/110.1 MiB.                                    \nCopying gs://tpu-pytorch/wheels/torch_xla-nightly-cp37-cp37m-linux_x86_64.whl...\n/ [1 files][127.2 MiB/127.2 MiB]                                                \nOperation completed over 1 objects/127.2 MiB.                                    \nCopying gs://tpu-pytorch/wheels/torchvision-nightly-cp37-cp37m-linux_x86_64.whl...\n- [1 files][  2.4 MiB/  2.4 MiB]                                                \nOperation completed over 1 objects/2.4 MiB.                                      \nProcessing ./torch-nightly-cp37-cp37m-linux_x86_64.whl\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch==nightly) (1.18.5)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch==nightly) (0.18.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==nightly) (3.7.4.1)\n\u001b[31mERROR: fastai 1.0.61 requires torchvision, which is not installed.\u001b[0m\n\u001b[31mERROR: kornia 0.3.1 has requirement torch==1.5.0, but you'll have torch 1.7.0a0+55ac240 which is incompatible.\u001b[0m\n\u001b[31mERROR: allennlp 1.0.0 has requirement torch<1.6.0,>=1.5.0, but you'll have torch 1.7.0a0+55ac240 which is incompatible.\u001b[0m\nInstalling collected packages: torch\nSuccessfully installed torch-1.7.0a0+55ac240\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.1 is available.\nYou should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\nProcessing ./torch_xla-nightly-cp37-cp37m-linux_x86_64.whl\nInstalling collected packages: torch-xla\nSuccessfully installed torch-xla-1.6+2c732f2\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.1 is available.\nYou should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\nProcessing ./torchvision-nightly-cp37-cp37m-linux_x86_64.whl\nRequirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from torchvision==nightly) (7.2.0)\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from torchvision==nightly) (1.7.0a0+55ac240)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision==nightly) (1.18.5)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch->torchvision==nightly) (3.7.4.1)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch->torchvision==nightly) (0.18.2)\nInstalling collected packages: torchvision\nSuccessfully installed torchvision-0.8.0a0+a75fdd4\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.1 is available.\nYou should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nThe following additional packages will be installed:\n  libgfortran4 libopenblas-base\nThe following NEW packages will be installed:\n  libgfortran4 libomp5 libopenblas-base libopenblas-dev\n0 upgraded, 4 newly installed, 0 to remove and 59 not upgraded.\nNeed to get 8550 kB of archives.\nAfter this operation, 97.6 MB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libgfortran4 amd64 7.5.0-3ubuntu1~18.04 [492 kB]\nGet:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libopenblas-base amd64 0.2.20+ds-4 [3964 kB]\nGet:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libopenblas-dev amd64 0.2.20+ds-4 [3860 kB]\nGet:4 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libomp5 amd64 5.0.1-1 [234 kB]\nFetched 8550 kB in 0s (19.9 MB/s) \ndebconf: delaying package configuration, since apt-utils is not installed\nSelecting previously unselected package libgfortran4:amd64.\n(Reading database ... 107745 files and directories currently installed.)\nPreparing to unpack .../libgfortran4_7.5.0-3ubuntu1~18.04_amd64.deb ...\nUnpacking libgfortran4:amd64 (7.5.0-3ubuntu1~18.04) ...\nSelecting previously unselected package libopenblas-base:amd64.\nPreparing to unpack .../libopenblas-base_0.2.20+ds-4_amd64.deb ...\nUnpacking libopenblas-base:amd64 (0.2.20+ds-4) ...\nSelecting previously unselected package libopenblas-dev:amd64.\nPreparing to unpack .../libopenblas-dev_0.2.20+ds-4_amd64.deb ...\nUnpacking libopenblas-dev:amd64 (0.2.20+ds-4) ...\nSelecting previously unselected package libomp5:amd64.\nPreparing to unpack .../libomp5_5.0.1-1_amd64.deb ...\nUnpacking libomp5:amd64 (5.0.1-1) ...\nSetting up libomp5:amd64 (5.0.1-1) ...\nSetting up libgfortran4:amd64 (7.5.0-3ubuntu1~18.04) ...\nSetting up libopenblas-base:amd64 (0.2.20+ds-4) ...\nupdate-alternatives: using /usr/lib/x86_64-linux-gnu/openblas/libblas.so.3 to provide /usr/lib/x86_64-linux-gnu/libblas.so.3 (libblas.so.3-x86_64-linux-gnu) in auto mode\nupdate-alternatives: using /usr/lib/x86_64-linux-gnu/openblas/liblapack.so.3 to provide /usr/lib/x86_64-linux-gnu/liblapack.so.3 (liblapack.so.3-x86_64-linux-gnu) in auto mode\nSetting up libopenblas-dev:amd64 (0.2.20+ds-4) ...\nupdate-alternatives: using /usr/lib/x86_64-linux-gnu/openblas/libblas.so to provide /usr/lib/x86_64-linux-gnu/libblas.so (libblas.so-x86_64-linux-gnu) in auto mode\nupdate-alternatives: using /usr/lib/x86_64-linux-gnu/openblas/liblapack.so to provide /usr/lib/x86_64-linux-gnu/liblapack.so (liblapack.so-x86_64-linux-gnu) in auto mode\nProcessing triggers for libc-bin (2.27-3ubuntu1) ...\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Dependencies"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"!pip install wtfml==0.0.3\n!pip install efficientnet_pytorch\n!pip install pytorch-lightning","execution_count":2,"outputs":[{"output_type":"stream","text":"Collecting wtfml==0.0.3\n  Downloading wtfml-0.0.3-py3-none-any.whl (10 kB)\nRequirement already satisfied: scikit-learn>=0.22.1 in /opt/conda/lib/python3.7/site-packages (from wtfml==0.0.3) (0.23.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.22.1->wtfml==0.0.3) (2.1.0)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.22.1->wtfml==0.0.3) (0.14.1)\nRequirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.22.1->wtfml==0.0.3) (1.4.1)\nRequirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.22.1->wtfml==0.0.3) (1.18.5)\nInstalling collected packages: wtfml\nSuccessfully installed wtfml-0.0.3\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.1 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\nCollecting efficientnet_pytorch\n  Downloading efficientnet_pytorch-0.6.3.tar.gz (16 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from efficientnet_pytorch) (1.7.0a0+55ac240)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet_pytorch) (3.7.4.1)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet_pytorch) (0.18.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet_pytorch) (1.18.5)\nBuilding wheels for collected packages: efficientnet-pytorch\n  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.6.3-py3-none-any.whl size=12419 sha256=fb89b0d4564fdc47d744bf30acbc94691424eec03f20967b815d85ca803bda51\n  Stored in directory: /root/.cache/pip/wheels/90/6b/0c/f0ad36d00310e65390b0d4c9218ae6250ac579c92540c9097a\nSuccessfully built efficientnet-pytorch\nInstalling collected packages: efficientnet-pytorch\nSuccessfully installed efficientnet-pytorch-0.6.3\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.1 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\nCollecting pytorch-lightning\n  Downloading pytorch_lightning-0.8.5-py3-none-any.whl (313 kB)\n\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 313 kB 401 kB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: tqdm>=4.41.0 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning) (4.45.0)\nRequirement already satisfied: torch>=1.3 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning) (1.7.0a0+55ac240)\nRequirement already satisfied: future>=0.17.1 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning) (0.18.2)\nRequirement already satisfied: numpy>=1.16.4 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning) (1.18.5)\nRequirement already satisfied: tensorboard>=1.14 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning) (2.2.2)\nRequirement already satisfied: PyYAML>=5.1 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning) (5.3.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.3->pytorch-lightning) (3.7.4.1)\nRequirement already satisfied: grpcio>=1.24.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning) (1.30.0)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning) (1.7.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning) (3.2.1)\nRequirement already satisfied: wheel>=0.26; python_version >= \"3\" in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning) (0.34.2)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning) (46.1.3.post20200325)\nRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning) (1.0.1)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning) (0.4.1)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning) (2.23.0)\nRequirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning) (1.14.0)\nRequirement already satisfied: protobuf>=3.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning) (3.12.2)\nRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning) (0.9.0)\nRequirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning) (1.14.0)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch-lightning) (1.2.0)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch-lightning) (1.24.3)\nRequirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch-lightning) (3.0.4)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch-lightning) (2.9)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch-lightning) (2020.6.20)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning) (3.1.1)\nRequirement already satisfied: rsa<4.1,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning) (4.0)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning) (0.2.7)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch-lightning) (3.0.1)\nRequirement already satisfied: pyasn1>=0.1.3 in /opt/conda/lib/python3.7/site-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning) (0.4.8)\nInstalling collected packages: pytorch-lightning\nSuccessfully installed pytorch-lightning-0.8.5\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.1 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\nimport os\nimport torch\nimport albumentations\n\nimport numpy as np\nimport pandas as pd\n\nimport torch.nn as nn\nfrom sklearn import metrics\nfrom sklearn import model_selection\nfrom torch.nn import functional as F\n\n# from wtfml.engine import Engine\n# from wtfml.utils import EarlyStopping\n# from wtfml.data_loaders.image import ClassificationDataLoader\nfrom torch.utils.data import Dataset,DataLoader\n\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.distributed.xla_multiprocessing as xmp\n\nimport efficientnet_pytorch\n\nfrom pytorch_lightning import LightningModule,Trainer\nfrom pytorch_lightning.callbacks import EarlyStopping,Callback\n\nfrom PIL import Image\nfrom pathlib import Path\nfrom tqdm import trange","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MEAN = [0.80619959, 0.62115946, 0.59133584]\nSTD = [0.15061945, 0.17709774, 0.20317172]","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_aug(train = False):\n    mean = MEAN\n    std = STD\n    if train:\n        train_aug = albumentations.Compose(\n        [albumentations.Normalize(mean, std, max_pixel_value=255.0, always_apply=True),\n        albumentations.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=15),\n            albumentations.Flip(p=0.5)])\n        return train_aug\n    else:\n        valid_aug = albumentations.Compose(\n        [albumentations.Normalize(mean, std, max_pixel_value=255.0,always_apply=True)])\n        return valid_aug","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aug = get_aug(train=True)","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create folds\ndf = pd.read_csv(\"../input/siim-isic-melanoma-classification/train.csv\")\ndf[\"kfold\"] = -1    \ndf = df.sample(frac=1).reset_index(drop=True)\ny = df.target.values\nkf = model_selection.StratifiedKFold(n_splits=5)\n\nfor f, (t_, v_) in enumerate(kf.split(X=df, y=y)):\n    df.loc[v_, 'kfold'] = f\n\ndf.to_csv(\"train_folds.csv\", index=False)","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN_DIR = Path(\"../input/siic-isic-224x224-images/train/\")","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":25,"outputs":[{"output_type":"execute_result","execution_count":25,"data":{"text/plain":"         image_name  patient_id     sex  age_approx  \\\n0      ISIC_5234605  IP_1583136  female        65.0   \n1      ISIC_4419280  IP_1362494  female        50.0   \n2      ISIC_9042665  IP_4391034  female        55.0   \n3      ISIC_9189319  IP_1800426  female        40.0   \n4      ISIC_7207496  IP_7817798  female        30.0   \n...             ...         ...     ...         ...   \n33121  ISIC_2008230  IP_7160012    male        60.0   \n33122  ISIC_7658268  IP_7279968    male        45.0   \n33123  ISIC_6749926  IP_0097257  female        65.0   \n33124  ISIC_5148638  IP_9453080  female        60.0   \n33125  ISIC_6151367  IP_7817315  female        60.0   \n\n      anatom_site_general_challenge diagnosis benign_malignant  target  kfold  \n0                             torso   unknown           benign       0      0  \n1                         head/neck   unknown           benign       0      0  \n2                             torso   unknown           benign       0      0  \n3                   upper extremity   unknown           benign       0      0  \n4                             torso     nevus           benign       0      0  \n...                             ...       ...              ...     ...    ...  \n33121                         torso   unknown           benign       0      4  \n33122                         torso   unknown           benign       0      4  \n33123               upper extremity   unknown           benign       0      4  \n33124               lower extremity   unknown           benign       0      4  \n33125                         torso   unknown           benign       0      4  \n\n[33126 rows x 9 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_name</th>\n      <th>patient_id</th>\n      <th>sex</th>\n      <th>age_approx</th>\n      <th>anatom_site_general_challenge</th>\n      <th>diagnosis</th>\n      <th>benign_malignant</th>\n      <th>target</th>\n      <th>kfold</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ISIC_5234605</td>\n      <td>IP_1583136</td>\n      <td>female</td>\n      <td>65.0</td>\n      <td>torso</td>\n      <td>unknown</td>\n      <td>benign</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ISIC_4419280</td>\n      <td>IP_1362494</td>\n      <td>female</td>\n      <td>50.0</td>\n      <td>head/neck</td>\n      <td>unknown</td>\n      <td>benign</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ISIC_9042665</td>\n      <td>IP_4391034</td>\n      <td>female</td>\n      <td>55.0</td>\n      <td>torso</td>\n      <td>unknown</td>\n      <td>benign</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ISIC_9189319</td>\n      <td>IP_1800426</td>\n      <td>female</td>\n      <td>40.0</td>\n      <td>upper extremity</td>\n      <td>unknown</td>\n      <td>benign</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ISIC_7207496</td>\n      <td>IP_7817798</td>\n      <td>female</td>\n      <td>30.0</td>\n      <td>torso</td>\n      <td>nevus</td>\n      <td>benign</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>33121</th>\n      <td>ISIC_2008230</td>\n      <td>IP_7160012</td>\n      <td>male</td>\n      <td>60.0</td>\n      <td>torso</td>\n      <td>unknown</td>\n      <td>benign</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>33122</th>\n      <td>ISIC_7658268</td>\n      <td>IP_7279968</td>\n      <td>male</td>\n      <td>45.0</td>\n      <td>torso</td>\n      <td>unknown</td>\n      <td>benign</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>33123</th>\n      <td>ISIC_6749926</td>\n      <td>IP_0097257</td>\n      <td>female</td>\n      <td>65.0</td>\n      <td>upper extremity</td>\n      <td>unknown</td>\n      <td>benign</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>33124</th>\n      <td>ISIC_5148638</td>\n      <td>IP_9453080</td>\n      <td>female</td>\n      <td>60.0</td>\n      <td>lower extremity</td>\n      <td>unknown</td>\n      <td>benign</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>33125</th>\n      <td>ISIC_6151367</td>\n      <td>IP_7817315</td>\n      <td>female</td>\n      <td>60.0</td>\n      <td>torso</td>\n      <td>unknown</td>\n      <td>benign</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n<p>33126 rows √ó 9 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def open_function(parent:Path, mode:str = \"RGB\", resize:int = None,):\n    def open_image(image_id:str):\n        img = Image.open(parent/f\"{image_id}.png\").convert(mode)\n        if resize:\n            img = img.resize((resize,resize))\n        return img\n    return open_image","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.kfold.value_counts()","execution_count":27,"outputs":[{"output_type":"execute_result","execution_count":27,"data":{"text/plain":"0    6626\n4    6625\n3    6625\n2    6625\n1    6625\nName: kfold, dtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"open_rgb = open_function(TRAIN_DIR,mode = \"RGB\")","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torchvision import transforms as trf","execution_count":29,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class siimData(Dataset):\n    def __init__(self,df,path,aug):\n        self.df = df\n        self.indices = df.index\n        self.path = Path(path)\n        self.aug = aug\n        \n    def __len__(self,):return len(self.df)\n        \n    def __getitem__(self,idx):\n        row = dict(self.df.loc[self.indices[idx]])\n        image_name = row[\"image_name\"]\n        target = row[\"target\"]\n        img = open_rgb(image_name)\n        \n        arr = self.aug(image = np.array(img))[\"image\"]\n        return np.moveaxis(arr,[0,1,2],[1,2,0]), target","execution_count":30,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Test dataloader"},{"metadata":{"trusted":true},"cell_type":"code","source":"dl = DataLoader(siimData(df,TRAIN_DIR,aug = get_aug(True)),batch_size=4)\nx,y = next(iter(dl))\nx.shape,y.shape","execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'DataLoader' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-30210c1fb7d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msiimData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTRAIN_DIR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maug\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_aug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'DataLoader' is not defined"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pytorch_lightning.metrics import Accuracy,F1\nfrom pytorch_lightning.callbacks import ","execution_count":17,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"invalid syntax (<ipython-input-17-7880f9eff42b>, line 2)","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-17-7880f9eff42b>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    from pytorch_lightning.callbacks import\u001b[0m\n\u001b[0m                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class EfficientNet(LightningModule):\n    def __init__(self,tag='efficientnet-b0',batch_size=16):\n        super(EfficientNet, self).__init__()\n        self.batch_size = batch_size\n        self.base_model = efficientnet_pytorch\\\n            .EfficientNet\\\n            .from_pretrained(tag)\n            \n        self.base_model._fc = nn.Linear(\n            in_features=1280, \n            out_features=1, \n            bias=True\n        )\n        self.acc = Accuracy(1)\n        self.f1 = F1(1)\n        \n        self.crit = nn.BCEWithLogitsLoss()\n        \n    def forward(self, image):\n        out = self.base_model(image)\n        return out\n    \n    def training_step(self,batch,batch_idx):\n        image, targets = batch\n        out = self(image)\n        loss = self.crit(out, targets.view(-1, 1).type_as(out))\n        log = {\"f1\":self.f1(out,targets),\"acc\":self.acc(out,targets)}\n        return {\"loss\":loss,\"log\":log}\n    \n    def validation_step(self,batch,batch_idx):\n        image, targets = batch\n        out = self(image)\n        loss = self.crit(out, targets.view(-1, 1).type_as(out))\n        log = {\"f1\":self.f1(out,targets),\"acc\":self.acc(out,targets)}\n        return {\"loss\":loss,\"log\":log}\n    \n    def get_sampler(self,ds,shuffle):\n        # required for TPU support\n        sampler = torch.utils.data.distributed.DistributedSampler(\n                    ds,\n                    num_replicas=xm.xrt_world_size(),\n                    rank=xm.get_ordinal(),\n                    shuffle=True\n                )\n        return sampler\n    \n    def train_dataloader(self):\n        aug = get_aug(True)\n        ds = siimData(self.train_df,TRAIN_DIR,aug = aug)\n        sampler = self.get_sampler(ds,shuffle=True)\n        dl = DataLoader(ds,sampler = sampler,batch_size = self.batch_size,num_workers = 2)\n        return dl\n    \n    def val_dataloader(self):\n        aug = get_aug(False)\n        ds = siimData(self.valid_df,TRAIN_DIR,aug = aug)\n        sampler = self.get_sampler(ds,shuffle=False)\n        dl = DataLoader(ds,sampler = sampler,batch_size = self.batch_size,num_workers = 2)\n        return dl\n    \n    def configure_optimizers(self):\n        return torch.optim.AdamW(self.parameters(),1e-4)\n    \n    @classmethod\n    def from_fold(cls,tag,df,kfold,batch_size = 16):\n        obj = cls(tag,batch_size=batch_size)\n        obj.train_df = df[df[\"kfold\"]!=kfold].sample(frac=1.).reset_index(drop=True)\n        obj.valid_df = df[df[\"kfold\"]==kfold].sample(frac=1.).reset_index(drop=True)\n        return obj","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"net = EfficientNet.from_fold('efficientnet-b0',df = df,kfold = 0,batch_size = 64)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Trainer"},{"metadata":{"trusted":true},"cell_type":"code","source":"trainer = Trainer(max_epochs=20,tpu_cores=8,distributed_backend=\"ddp\",replace_sampler_ddp = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainer.fit(net)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# init model here\n# MX = EfficientNet()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train():\n    training_data_path = \"../input/siic-isic-224x224-images/train/\"\n    df = pd.read_csv(\"/kaggle/working/train_folds.csv\")\n    device = xm.xla_device()\n    epochs = 5\n    train_bs = 32\n    valid_bs = 16\n    fold = 0\n\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n\n    model = MX.to(device)\n\n    mean = (0.485, 0.456, 0.406)\n    std = (0.229, 0.224, 0.225)\n    train_aug = albumentations.Compose(\n        [\n            albumentations.Normalize(\n                mean, \n                std, \n                max_pixel_value=255.0, \n                always_apply=True\n            ),\n            albumentations.ShiftScaleRotate(\n                shift_limit=0.0625, \n                scale_limit=0.1, \n                rotate_limit=15\n            ),\n            albumentations.Flip(p=0.5)\n        ]\n    )\n\n    valid_aug = albumentations.Compose(\n        [\n            albumentations.Normalize(\n                mean, \n                std, \n                max_pixel_value=255.0,\n                always_apply=True\n            )\n        ]\n    )\n\n    train_images = df_train.image_name.values.tolist()\n    train_images = [\n        os.path.join(training_data_path, i + \".png\") for i in train_images\n    ]\n    train_targets = df_train.target.values\n\n    valid_images = df_valid.image_name.values.tolist()\n    valid_images = [\n        os.path.join(training_data_path, i + \".png\") for i in valid_images\n    ]\n    valid_targets = df_valid.target.values\n\n    train_loader = ClassificationDataLoader(\n        image_paths=train_images,\n        targets=train_targets,\n        resize=None,\n        augmentations=train_aug,\n    ).fetch(\n        batch_size=train_bs, \n        drop_last=True, \n        num_workers=4, \n        shuffle=True, \n        tpu=True\n    )\n\n    valid_loader = ClassificationDataLoader(\n        image_paths=valid_images,\n        targets=valid_targets,\n        resize=None,\n        augmentations=valid_aug,\n    ).fetch(\n        batch_size=valid_bs, \n        drop_last=False, \n        num_workers=2, \n        shuffle=False, \n        tpu=True\n    )\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer,\n        patience=3,\n        threshold=0.001,\n        mode=\"min\"\n    )\n\n    es = EarlyStopping(patience=5, mode=\"min\", tpu=True)\n    eng = Engine(model, optimizer, device=device, use_tpu=True, tpu_print=25)\n\n    for epoch in range(epochs):\n        train_loss = eng.train(train_loader)\n        valid_loss = eng.evaluate(valid_loader)\n        xm.master_print(f\"Epoch = {epoch}, LOSS = {valid_loss}\")\n        scheduler.step(valid_loss)\n\n        es(valid_loss, model, model_path=f\"model_fold_{fold}.bin\")\n        if es.early_stop:\n            xm.master_print(\"Early stopping\")\n            break\n        gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def _mp_fn(rank, flags):\n    torch.set_default_tensor_type('torch.FloatTensor')\n    a = train()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"FLAGS={}\nxmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method='fork')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Recalculate mean and std"},{"metadata":{"trusted":true},"cell_type":"code","source":"# def img2arr(path):\n#     path = Path(path)\n#     def open_img(img_id):\n#         return np.array(Image.open(path/f\"{img_id}.png\"))/255\n#     return open_img\n\n# means = []\n# stds = []\n# images = []\n# image_names = list(df.image_name)\n# open_train = img2arr(TRAIN_DIR)\n# with torch.no_grad():\n#     for i in trange(len(image_names)):\n#         img = image_names[i]\n#         images.append(open_train(img))\n#         if len(images)==320:\n#             concatenated = np.concatenate(images,axis=0).reshape(-1,3)\n#             means.append(concatenated.mean(0))\n#             stds.append(concatenated.std(0))\n#             images = []\n\n# np.stack(means,axis=0).mean(0),np.stack(stds,axis=0).mean(0)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat":4,"nbformat_minor":4}